}
variance_value <- sum_sq_diff / (length(data) - 1)
# Calculate standard deviation
std_dev_value <- sqrt(variance_value)
print(paste("Standard Deviation: ", std_dev_value))
# Create a numeric vector
data1 <- read.csv("employees1.xlsx")
install.packages("xlsx")
library(xlsx)
data1 <- read.xlsx("employees1.xlsx", sheetName = "Sheet1")
# Create a numeric vector
#data1 <- read.csv("employees1.xlsx")
# Get the Salary from data frame.
data <- data1$Salary
# Calculate mean
sum <- 0
for(i in data){
sum <- sum + i
}
mean_value <- sum / length(data)
print(paste("Mean: ", mean_value))
# Calculate median
sorted_data <- sort(data)
n <- length(sorted_data)
if(n %% 2 == 1){
median_value <- sorted_data[(n+1)/2]
} else {
median_value <- (sorted_data[n/2] + sorted_data[n/2 + 1]) / 2
}
print(paste("Median: ", median_value))
# Calculate mode
counts <- table(data)
mode_value <- as.numeric(names(counts)[counts == max(counts)])
print(paste("Mode: ", mode_value))
# Calculate variance
sum_sq_diff <- 0
for(i in data){
sum_sq_diff <- sum_sq_diff + (i - mean_value)^2
}
variance_value <- sum_sq_diff / (length(data) - 1)
# Calculate standard deviation
std_dev_value <- sqrt(variance_value)
print(paste("Standard Deviation: ", std_dev_value))
# Number of random samples
n <- 1000
# Create vectors to store the generated samples
x <- numeric(n)
y <- numeric(n)
# Generate random samples using the Box-Muller transform
for (i in 1:(n/2)) {
u1 <- runif(1)
u2 <- runif(1)
r <- sqrt(-2 * log(u1))
theta <- 2 * pi * u2
x[i] <- r * cos(theta)
y[i] <- r * sin(theta)
}
# Combine the two sets of samples to get a standard normal distribution
z <- c(x, y)
# Plot a histogram of the generated normal distribution
hist(z, breaks = 30, main = "Generated Normal Distribution", xlab = "Value")
# Number of random samples
n <- 250
# Create vectors to store the generated samples
x <- numeric(n)
y <- numeric(n)
# Generate random samples using the Box-Muller transform
for (i in 1:(n/2)) {
u1 <- runif(1)
u2 <- runif(1)
r <- sqrt(-2 * log(u1))
theta <- 2 * pi * u2
x[i] <- r * cos(theta)
y[i] <- r * sin(theta)
}
# Combine the two sets of samples to get a standard normal distribution
z <- c(x, y)
# Plot a histogram of the generated normal distribution
hist(z, breaks = 30, main = "Generated Normal Distribution", xlab = "Value")
# Number of random samples
n <- 1000
# Create vectors to store the generated samples
x <- numeric(n)
y <- numeric(n)
# Generate random samples using the Box-Muller transform
for (i in 1:(n/2)) {
u1 <- runif(1)
u2 <- runif(1)
r <- sqrt(-2 * log(u1))
theta <- 2 * pi * u2
x[i] <- r * cos(theta)
y[i] <- r * sin(theta)
}
# Combine the two sets of samples to get a standard normal distribution
z <- c(x, y)
# Plot a histogram of the generated normal distribution
hist(z, breaks = 30, main = "Generated Normal Distribution", xlab = "Value")
# Get the data points in form of a R vector.
rainfall <- c(799,1174.8,865.1,1334.6,635.4,918.5,685.5,998.6,784.2,985,882.8,1071)
# Convert it to a time series object.
rainfall.timeseries <- ts(rainfall,start = c(2012,1),frequency = 12)
# Print the timeseries data.
print(rainfall.timeseries)
# Give the chart file a name.
#png(file = "rainfall.png")
# Plot a graph of the time series.
plot(rainfall.timeseries)
sp<-read.table('E:/MCA/Unit iv/stock_daily.csv', sep=",", header= TRUE)
sp<-read.table('stock_daily.csv', sep=",", header= TRUE)
sp_ts <- ts(sp, start=2016, frequency=365)
plot.ts(sp_ts,col=4, main="Daily Stock Prices", ylab="Price")
#We can observe two things here:
# Read the data from the CSV file
sp <- read.table('stock_daily.csv', sep = ",", header = TRUE)
# Create a time series object with a custom date sequence
dates <- seq(as.Date("2016-01-01"), by = "days", length.out = nrow(sp))
sp_ts <- ts(sp$Price, start = c(year(dates[1]), month(dates[1]), day(dates[1])),
frequency = 365)
# Read the data from the CSV file
sp <- read.table('stock_daily.csv', sep = ",", header = TRUE)
dates <- seq(as.Date("2016-01-01"), by = "days", length.out = nrow(sp))
sp_ts <- ts(sp$Price, start = c(year(dates[1]), month(dates[1]), day(dates[1])),
frequency = 365)
View(sp)
View(sp)
sp_ts <- ts(sp, start=2016, frequency=365)
# Create a multi-panel plot for better visualization
par(mfrow = c(2, 1))
# Plot the daily stock prices
plot.ts(sp_ts, col = "blue", main = "Daily Stock Prices", ylab = "Price")
# Add a second panel with volume data (assuming "Volume" is a column in your data)
plot.ts(ts(sp$Volume, start = start(sp_ts), frequency = frequency(sp_ts)),
col = "green", main = "Trading Volume", ylab = "Volume")
# Load the stock data from a CSV file into a data frame
sp <- read.table('stock_daily.csv', sep=",", header=TRUE)
# Create a time series object (ts) from the data
sp_ts <- ts(sp, start=2016, frequency=365)
# Plot the time series data
plot.ts(sp_ts, col=4, main="Daily Stock Prices", ylab="Price")
# Load the stock data from a CSV file into a data frame
sp <- read.table('stock_daily.csv', sep=",", header=TRUE)
# Create a time series object (ts) from the data
sp_ts <- ts(sp, start=2016, frequency=365)
# Plot the time series data
plot.ts(sp_ts, col=4, main="Daily Stock Prices", ylab="Price")
# Load the stock data from a CSV file into a data frame
sp <- read.table('stock_daily.csv', sep=",", header=TRUE)
# Create a time series object (ts) from the data
sp_ts <- ts(sp, start=2016, frequency=365)
# Plot the time series data
plot.ts(sp_ts, col=4, main="Daily Stock Prices", ylab="Price")
sp<-read.table('stock_daily.csv', sep=",", header= TRUE)
sp_ts <- ts(sp, start=2016, frequency=365)
plot.ts(sp_ts,col=4, main="Daily Stock Prices", ylab="Price")
sp<-read.table('stock_daily.csv', sep=",", header= TRUE)
sp_ts <- ts(sp, start=2016, frequency=365)
plot.ts(sp_ts,col=4, main="Daily Stock Prices", ylab="Price")
sp<-read.table('stock_daily.csv', sep=",", header= TRUE)
sp_ts <- ts(sp, start=2016, frequency=365)
plot.ts(sp_ts,col=4, main="Daily Stock Prices", ylab="Price")
sp<-read.table('stock_daily.csv', sep=",", header= TRUE)
sp_ts <- ts(sp, start=2016, frequency=365)
plot.ts(sp_ts,col=4, main="Daily Stock Prices", ylab="Price")
sp<-read.table('stock_daily.csv', sep=",", header= TRUE)
sp_ts <- ts(sp, start=2016, frequency=365)
plot.ts(sp_ts,col=4, main="Daily Stock Prices", ylab="Price")
# Load the stock data from a CSV file into a data frame
sp <- read.table('stock_daily.csv', sep=",", header=TRUE)
# Create a time series object (ts) from the data
sp_ts <- ts(sp, start=2016, frequency=365)
# Plot the time series data
plot.ts(sp_ts, col=4, main="Daily Stock Prices", ylab="Price")
# Complete ARIMA model for Air passengers
#http://rstudio-pubs-static.s3.amazonaws.com/311446_08b00d63cc794e158b1f4763eb70d43a.html
#Time Series Analysis and Modeling with the Air Passengers Dataset
#Synopsis
#This objective of this analysis and modelling is to review time series theory and experiment with R packages.
#We will be following an ARIMA modeling procedure of the AirPassengers dataset as follows:
#  1. Perform exploratory data analysis
#2. Decomposition of data
#3. Test the stationarity
#4. Fit a model used an automated algorithm
#5. Calculate forecasts
library(ggfortify)
library(tseries)
library(forecast)
data(AirPassengers)
head(AirPassengers)
print(AirPassengers)
AP <- AirPassengers
# Take a look at the class of the dataset AirPassengers
class(AP)
#The AirPassenger dataset in R provides monthly totals of a US airline passengers,
#from 1949 to 1960. This dataset is already of a time series class therefore
#no further class or date manipulation is required.
#To perform exploratory analysis, let’s first review the data with summary
#statistics and plots in R.
# Take a look at the entries
AP
# Check for missing values
sum(is.na(AP))
# Check the frequency of the time series
frequency(AP)
# Check the cycle of the time series
cycle(AP)
# Review the table summary
summary(AP)
# Plot the raw data using the base plot function
plot(AP,xlab="Date", ylab = "Passenger numbers (1000's)",main="Air Passenger numbers from 1949 to 1961")
#As an alternative to the base plot function, so we can also use the extension ggfortify
#R package from the ggplot2 package, to plot directly from a time series.
#The benefits are not having to convert to a dataframe as required with ggplot2,
#but still having access to the layering grammar of graphics.
library(ggfortify)
autoplot(AP) + labs(x ="Date", y = "Passenger numbers (1000's)", title="Air Passengers from 1949 to 1961")
#Let’s use the boxplot function to see any seasonal effects.
boxplot(AP~cycle(AP),xlab="Date", ylab = "Passenger Numbers (1000's)" ,main ="Monthly Air Passengers Boxplot from 1949 to 1961")
#From these exploratory plots, we can make some initial inferences:
# 1. The passenger numbers increase over time with each year which may be
# indicative of an increasing linear trend, perhaps due to increasing demand for flight
#  travel and commercialisation of airlines in that time period.
#2. In the boxplot there are more passengers travelling in months 6 to 9
#with higher means and higher variances than the other months,
#indicating seasonality with a apparent cycle of 12 months.
#The rationale for this could be more people taking holidays and
#fly over the summer months in the US.
#3. AirPassengers appears to be multiplicative time series as the passenger numbers
#increase, it appears so does the pattern of seasonality.
#4. There do not appear to be any outliers and there are no missing values.
#Therefore no data cleaning is required.
#Part 2: TIME SERIES DECOMPOSITION
#We will decompose the time series for estimates of trend, seasonal, and
#random components using moving average method.
#The multiplicative model is:
#  Y[t]=T[t]∗S[t]∗e[t]
#where
#Y(t) is the number of passengers at time t,
#T(t) is the trend component at time t,
#S(t) is the seasonal component at time t,
#e(t) is the random error component at time t.
#With this model, we will use the decompose function in R. Continuing to use ggfortify
#for plots, in one line, autoplot these decomposed components to further analyse the data.
decomposeAP <- decompose(AP,"multiplicative")
autoplot(decomposeAP)
decomposeAP<-decompose(AP,"additive")
autoplot(decomposeAP)
#In these decomposed plots we can again see the trend and seasonality as inferred previously,
#but we can also observe the estimation of the random component depicted under the “remainder”.
#Part 3: TEST STATIONARITY OF THE TIME SERIES
#A stationary time series has the conditions that the mean,
#variance and covariance are not functions of time. In order to fit arima models,
#the time series is required to be stationary.
#We will use two methods to test the stationarity.
#1. Test stationarity of the time series (ADF)
#In order to test the stationarity of the time series, let’s run the Augmented
#Dickey-Fuller Test using the adf.test function from the tseries R package.
#First set the hypothesis test:
#  The null hypothesis H0 : that the time series is non stationary
#The alternative hypothesis HA : that the time series is stationary
adf.test(AP)
#As a rule of thumb, where the p-value is less than 5%, we strong evidence against
#the null hypothesis, so we reject the null hypothesis. As per the test results above,
#the p-value is 0.01 which is <0.05 therefore we reject the null in favour of the
#alternative hypothesis that the time series is stationary.
#2. Test stationarity of the time series (Autocorrelation)
#Another way to test for stationarity is to use autocorrelation. We will use autocorrelation
#function (acf) in from the base stats R package. This function plots the correlation
#between a series and its lags ie previous observations with a 95% confidence interval in blue.
#If the autocorrelation crosses the dashed blue line, it means that specific lag is significantly
#correlated with current series.
autoplot(acf(AP,plot=FALSE))+ labs(title="Correlogram of Air Passengers from 1949 to 1961")
#The maximum at lag 1 or 12 months, indicates a positive relationship with the 12 month cycle.
#Since we have already created the decomposeAP list object with a random component,
#we can plot the acf of the decomposeAP$random.
# Review random time series for any missing values
decomposeAP$random
# Autoplot the random time series from 7:138 which exclude the NA values
autoplot(acf(decomposeAP$random[7:138],plot=FALSE))+ labs(title="Correlogram of Air Passengers Random Component from 1949 to 1961")
#We can see that the acf of the residuals is centered around 0.
#Part 4: FIT A TIME SERIES MODEL
#1. Linear Model
#Since there is an upwards trend we will look at a linear model first for comparison.
#We plot AirPassengers raw dataset with a blue linear model.
library(ggplot2)
autoplot(AP) + geom_smooth(method="lm",se=F)+ labs(x ="Date", y = "Passenger numbers (1000's)", title="Air Passengers from 1949 to 1961")
#This may not be the best model to fit as it doesn’t capture the seasonality and
#multiplicative effects over time.
#2. ARIMA Model
#Use the auto.arima function from the forecast R package to fit the best model and
#coefficients, given the default parameters including seasonality as TRUE. Note we have
#used the ARIMA modeling procedure as referenced
arimaAP <- auto.arima(AP)
arimaAP
#The ARIMA(2,1,1)(0,1,0)[12] model parameters are lag 1 differencing (d),
#an autoregressive term of second lag (p) and a moving average model of order 1 (q).
#Then the seasonal model has an autoregressive term of first lag (D) at model period 12 units,
#in this case months.
#The ARIMA fitted model is:
#  Y^=0.5960Yt−2+0.2143Yt−12−0.9819et−1+E
#where E is some error.
#The ggtsdiag function from ggfortify R package performs model diagnostics of
#the residuals and the acf. will include a autocovariance plot.
ggtsdiag(arimaAP)
#Part 5: CALCULATE FORECASTS
#Finally we can plot a forecast of the time series using the forecast function,
#again from the forecast R package, with a 95% confidence interval where h is the forecast
#horizon periods in months.
forecastAP <- forecast(arimaAP, level = c(95), h = 36)
autoplot(forecastAP)
#To summarize, this has been an exercise in ARIMA modeling and using time series R packages
#ggfortify, tseries and forecast.
#It is a good basis to move on to more complicated time series datasets,
#models and comparisons in R.
#Another way of forecasting airpassenger data
A <- arima(AirPassengers, order = c(2,1,1),seasonal=c(0,1,0))
library(forecast)
forecast <- forecast(A, h = 30) # predict 30 years into the future
plot(forecast)
library(ggfortify)
library(tseries)
library(forecast)
data(AirPassengers)
head(AirPassengers)
print(AirPassengers)
AP <- AirPassengers
# Take a look at the class of the dataset AirPassengers
class(AP)
# Plot the raw data using the base plot function
plot(AP,xlab="Date", ylab = "Passenger numbers (1000's)",main="Air Passenger numbers from 1949 to 1961")
arimaAP <- auto.arima(AP)
arimaAP
forecastAP <- forecast(arimaAP, level = c(95), h = 36)
autoplot(forecastAP)
#Another way of forecasting airpassenger data
A <- arima(AirPassengers, order = c(2,1,1),seasonal=c(0,1,0))
library(forecast)
forecast <- forecast(A, h = 30) # predict 30 years into the future
plot(forecast)
setwd("E:/GitHub/MCA/DataScience_R_Lab/Datascience using R Sridhar notes/Datascience using R/Unit iv")
n plot multiple time series in one chart by combining both the series into a matrix.
#Live Demo
# Get the data points in form of a R vector.
rainfall1 <- c(799,1174.8,865.1,1334.6,635.4,918.5,685.5,998.6,784.2,985,882.8,1071)
rainfall2 <- c(655,1306.9,1323.4,1172.2,562.2,824,822.4,1265.5,799.6,1105.6,1106.7,1337.8)
# Convert them to a matrix.
combined.rainfall <-  matrix(c(rainfall1,rainfall2),nrow = 12)
# Convert it to a time series object.
rainfall.timeseries <- ts(combined.rainfall,start = c(2012,1),frequency = 12)
# Print the timeseries data.
print(rainfall.timeseries)
# Give the chart file a name.
#png(file = "rainfall_combined.png")
# Plot a graph of the time series.
plot(rainfall.timeseries, main = "Multiple Time Series")
weatherdata <- read.table('daily-min-temperatures.csv', sep=",", header= TRUE)
head(weatherdata)
plot.ts(weatherdata$Temp, main="Weather", ylab="Temperature")
plot.ts(log(weatherdata$Temp),main="Weather", ylab="Temperature")
temperature.timeseries <- ts(weatherdata$Temp)
plot.ts(temperature.timeseries)
datats=ts(weatherdata,start = c(2012,1),frequency = 12)
plot(decompose(datats))
msft_ar <- arima(temperature.timeseries, order = c(1, 2, 2))
print(msft_ar)
library(forecast)
pred<-forecast(msft_ar,h=365)
plot(pred)
library(forecast)
pred<-forecast(msft_ar,h=365)
plot(pred)
sp<-read.table('stock_daily.csv', sep=",", header= TRUE)
print(sp)
sp_ts <- ts(sp, start=2016, frequency=365)
print(sp_ts)
plot.ts(sp_ts,col=10, main="Daily Stock Prices", ylab="Price")
#We can observe two things here:
plot(pred)
sp_linear<-log(sp_ts)
plot.ts(sp_linear, main="Daily Stock Prices (log)", ylab="Price", col=4)
par(mfrow = c(1,2))
plot.ts(sp_ts,col=4, main="Daily Stock Prices", ylab="Price")
plot.ts(sp_linear, main="Daily Stock Prices (log)", ylab="Price", col=4)
#---------------
sp_linear_diff <- diff(sp_linear)
plot.ts(sp_linear_diff, main="Daily Stock Prices (log)", ylab="Price", col=4)
par(mfrow = c(1,2))
de_earnings_diff <- diff(johndeere_earnings,lag=4)
#d) Stock Analysis
library(forecast)
pred<-forecast(msft_ar,h=365)
plot(pred)
sp<-read.table('stock_daily.csv', sep=",", header= TRUE)
print(sp)
sp_ts <- ts(sp, start=2016, frequency=365)
print(sp_ts)
plot.ts(sp_ts,col=10, main="Daily Stock Prices", ylab="Price")
#d) Stock Analysis
library(forecast)
pred<-forecast(msft_ar,h=365)
plot(pred)
sp<-read.table('stock_daily.csv', sep=",", header= TRUE)
print(sp)
sp_ts <- ts(sp, start=2016, frequency=365)
print(sp_ts)
plot.ts(sp_ts,col=10, main="Daily Stock Prices", ylab="Price")
#e) Daily Temperature
weatherdata <- read.table('daily-min-temperatures.csv', sep=",", header= TRUE)
head(weatherdata)
plot.ts(weatherdata$Temp, main="Weather", ylab="Temperature")
plot.ts(log(weatherdata$Temp),main="Weather", ylab="Temperature")
temperature.timeseries <- ts(weatherdata$Temp)
plot.ts(temperature.timeseries)
datats=ts(weatherdata,start = c(2012,1),frequency = 12)
plot(decompose(datats))
plot.ts(temperature.timeseries)
library(TTR)
x <- c(3,5,7,3,4,2,6,4,7,2,1,9, 1, 10, 1,12)
y=x
By <- diff(y) # y_i - B y_i
print(By)
B2y<-diff(y,2)
B3y <- diff(y, 3) # y_i - B^3 y_i
message(paste0("y is: ", paste(y, collapse = ","), "\n",
"By is: ", paste(By, collapse = ","), "\n",
"By2 is: ",paste(B2y, collapse = ","), "\n",
"B3y is: ", paste(B3y, collapse = ",")))
get_autocor <- function(x, lag) {
x.left <- x[1:(length(x) - lag)]
x.right <- x[(1+lag):(length(x))]
print(x.left)
print(x.right)
autocor <- cor(x.left, x.right)
return(autocor)
}
# correlation of measurements 1 time point apart (lag 1)
get_autocor(y, 1)
# correlation of measurements 2 time points apart (lag 2)
get_autocor(y, 2)
get_autocor(y,3)
# correlation of measurements 2 time points apart (lag 2)
get_autocor(y, 4)
#Moving Average
print(x)
# 3 5 7 3 4 2 6 4
filter(x, rep(1/3,3))
#4.0 6.0 5.0 3.5 3.0 4.0 5.0 5.5 4.5 1.5
#gives rolling mean of every 2 consecutive observations
runMean(x,2)
head(weatherdata)
plot(weatherdata$Temp)
plot(log(weatherdata$Temp))
par(mfrow = c(1,2))
acf(weatherdata$Temp) # conventional ACF
pacf(weatherdata$Temp) # pACF
# Load the data
weatherdata <- read.table('daily-min-temperatures.csv', sep=",", header=TRUE)
head(weatherdata)
# Convert the data to a time series object
weather_ts <- ts(weatherdata$Temp, start=c(1981, 1), frequency=365)
# Plot the original data
plot(weather_ts, main="Daily Minimum Temperatures")
# Moving Average
moving_avg <- ma(weather_ts, order=7)  # 7-day moving average
plot(moving_avg, main="7-Day Moving Average")
# Moving Average
moving_avg <- ma(weather_ts, order=7)  # 7-day moving average
plot(moving_avg, main="7-Day Moving Average")
# Auto Regression
library(forecast)
auto_reg <- Arima(weather_ts, order=c(5,0,0))  # AR(5) model
plot(auto_reg, main="Auto Regression (ARIMA)")
# ARIMA
arima_model <- auto_reg  # You can fine-tune ARIMA parameters
plot(forecast(arima_model), main="ARIMA Forecast")
# Auto Regression (AR)
library(forecast)
auto_reg <- ar(weather_ts, aic=TRUE, order.max=10)  # Fit an AR model
order <- auto_reg$order
plot(auto_reg, main=paste("Auto Regression (AR(", order, "))"), xlab="Lag", ylab="ACF")
# Auto Regression (AR)
library(forecast)
auto_reg <- ar(weather_ts, aic=TRUE, order.max=10)  # Fit an AR model
order <- auto_reg$order
# Plot the ACF as a bar plot
acf_values <- acf(weather_ts, plot = FALSE)$acf
barplot(acf_values, main = paste("Auto Regression (AR(", order, ")) ACF Bar Plot"), xlab = "Lag", ylab = "ACF")
# Auto Regression (AR)
library(forecast)
auto_reg <- ar(weather_ts, aic=TRUE, order.max=10)  # Fit an AR model
order <- auto_reg$order
# Plot the ACF as a bar plot
acf_values <- acf(weather_ts, plot = FALSE)$acf
barplot(acf_values, main = paste("Auto Regression (AR(", order, ")) ACF Bar Plot"), xlab = "Lag", ylab = "ACF")
# Auto Regression (AR)
library(forecast)
auto_reg <- ar(weather_ts, aic=TRUE, order.max=10)  # Fit an AR model
order <- auto_reg$order
# Compute the ACF values
acf_values <- acf(weather_ts, plot = FALSE)$acf
# Plot the ACF as a bar plot
barplot(as.vector(acf_values), main = paste("Auto Regression (AR(", order, ")) ACF Bar Plot"), xlab = "Lag", ylab = "ACF")
library(forecast)
auto_reg <- Arima(weather_ts, order=c(5,0,0))  # AR(5) model
plot(auto_reg, main="Auto Regression (ARIMA)")
arima_model <- auto_reg  # You can fine-tune ARIMA parameters
plot(forecast(arima_model), main="ARIMA Forecast")
setwd("E:/GitHub/MCA/DataScience_R_Lab/R Lab Changes/R Changes")
#Another way of forecasting airpassenger data
A <- arima(AirPassengers, order = c(2,1,1),seasonal=c(0,1,0))
library(forecast)
forecast <- forecast(A, h = 30) # predict 30 years into the future
plot(forecast)
